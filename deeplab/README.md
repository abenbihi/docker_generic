## Building Deeplab caffe
Build only 

    # Build 
    make build

Build and run 
    
    # Set the external volume you want to mount
    EXTERNAL_PATH_TO_WS=external_volume

    # Set the internal volume you want to mount
    LOCAL_PATH_TO_WS=internal_volume

    # Build and run 
    make root

## Training deeplab

### Download the running script and the dataset

The scripts used for training and post-processing can be downloaded from this [link](https://ucla.app.box.com/s/4grlj8yoodv95936uybukjh5m0tdzvrf) :
	
	# the script for training/testing on the PASCAL VOC 2012 dataset. Note You also need to download sub.sed script.
    run_pascal.sh
	# the scripts we used for post-processing the DCNN computed results by DenseCRF.
	run_densecrf.sh 
	run_densecrf_grid_search.sh

The image list files used in our experiments can be downloaded from this [link](https://ucla.app.box.com/s/rd9z2xvwsfpksi7mi08i2xqrj7ab4keb):

The datasets can be found at these links:
- [Augmented PASCAL VOC: VOC_aug](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz) 1.3 GB
- [original PASCAL VOC 2012: VOC2012_orig](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar) 2GB

### Data conversion
Let $DATASETS be the directory holding your dataset.
Download the conversion scripts 'mat2png.py' and 'convert_labels.py' from this [link](https://github.com/martinkersner/train-DeepLab)

Unfortunately, ground truth labels within augmented PASCAL VOC dataset are distributed as Matlab data files, therefore we will have to convert them before we can start training itself.

	cd $DATASETS/VOC_aug/dataset
	mkdir cls_png
	cd $DEEPLAB
	./mat2png.py $DATASETS/VOC_aug/dataset/cls $DATASETS/VOC_aug/dataset/cls_png

Caffe softmax loss function can accept only one-channel ground truth labels. However, those labels in original PASCAL VOC 2012 dataset are defined as RGB images. Thus, we have to reduce their dimensionality.

	cd $DATASETS/VOC2012_orig
	mkdir SegmentationClass_1D

	./convert_labels.py $DATASETS/VOC2012_orig/SegmentationClass/ \
	  $DATASETS/VOC2012_orig/ImageSets/Segmentation/trainval.txt \
	  $DATASETS/VOC2012_orig/SegmentationClass_1D/

At last, part of code which computes DenseCRF is able to work only with PPM image files, hence we have to perform another conversion. This step is necessary only if we want to use DenseCRF separately and as one of Caffe layers.

    # DO NOT USE ANY OTHER CODE TO DO THE CONVERSION !!!! 
    # the crf script accepts only the ppm images generated with this code.
    # Don't ask why.
	# Modify the path variables and run the following script in matlab:
    deeplab/code/densecrf/my_script/SaveJpgToPPM.m

### Generate features map to feed crf: this creates fcn8 in deeplab/exper/voc12/features/deep_largeFOV/val/fc8
    # feature map in .mat format
    cd exper
    # Assuming you have generated test_val.prototxt with run_pascal.sh 
    caffe test --model=voc12/config/deep_largeFOV/test_val.prototxt --weights=voc12/config/deep_largeFOV/train_iter_20000.caffemodel --gpu=0 --iterations=1449
    # The code freezes on the output line "Running 1449 iterations": it is
    normal. It is computing the feature maps on the 1449 images.
    # You can kill it once you see "data layer prefetch queue empty ..."

### Train deeplab with your dataset
    # solver_train.prototxt has been generated by run+pascal.sh from the solver template. 
    # It also generates train_train.prototxt from train.prototxt 
    # and test_train.prototxt from test.prototxt
    cd exper
    caffe train --solver=voc12/config/deep_largeFOV/solver_train.prototxt 

### Post process your feature maps using crf (... it is long)
    # This process your features maps with crf and save the segmentation result
    # into a binary file
    cd exper
    ./run_densecrf.sh

### Visualize your results
    # In matlab
    /home/gpu_user/assia/ws/tf/deeplab/code/densecrf/my_script/GetDenseCRFResult.m

## Set the caffe path in your docker image
    
    export PYTHONPATH=$PYTHONPATH:/opt/caffe/python

## Launch the image (assumed you have built the image)
--volume=external_path:local_path mounts external_path in local_path in the image
    
    nvidia-docker run --volume=/usr/users/ims/benbihi_ass/ws/abenbihi/ws:/home/ws -it -u root deeplab bash

## Caffe useful commands 

Finetuning: 
    
    caffe train -solver solver.prototxt -weights ../fcn8s-heavy-pascal.caffemodel
